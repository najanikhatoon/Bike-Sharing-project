{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "Y3lxredqlCYt",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "VfCC591jGiD4",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/najanikhatoon/Bike-Sharing-project/blob/main/Another_copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project Name - Bike Sharing Demand Prediction**\n",
        "\n",
        "### **Project Type- Regression**\n",
        "\n",
        "### **Contribution - Individual**\n",
        "## **Name - Najani khatoon**"
      ],
      "metadata": {
        "id": "u9824q_iDs9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MPKsGw8DnbhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqrRTeGRvNJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "RdnQzvtZvTk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "BgSykrMivp73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUSINESS PROBLEM OVERVIEW\n",
        "\n",
        "To improve mobility comfort, many urban cities are now offering rental bikes. Because it shortens the time people have to wait, the public must have access to the rental bike at the right time. At some point, ensuring that the city has a consistent supply of rental bikes becomes a major concern. The significant part is the expectation of bicycle count expected at every hour for the steady stockpile of rental bicycles.\n",
        "\n",
        "Bike sharing systems are a way to rent bikes where a network of locations automates the membership, rental, and bike return processes throughout a city. People can rent bikes from one location and return them to another or the same location as needed through these Bike Sharing systems. Individuals can lease a bicycle through memebership or on request premise. A citywide network of automated stores oversees this procedure.\n",
        "\n",
        "Based on historical usage patterns in relation to weather, time, and other data, we are forecasting bike sharing demand prediction for the Bike Sharing Program in Seoul in this dataset."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkkKXW-GE4uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Motivation**\n",
        "\n",
        "Several bike/scooter ride sharing facilities (e.g., Bird, Capital Bikeshare, Citi Bike) have started up lately especially in metropolitan cities like San Francisco, New York, Chicago and Los Angeles, and one of the most important problem from a business point of view is to predict the bike demand on any particular day. While having excess bikes results in wastage of resource (both with respect to bike maintenance and the land/bike stand required for parking and security), having fewer bikes leads to revenue loss (ranging from a short term loss due to missing out on immediate customers to potential longer term loss due to loss in future customer base), Thus, having a estimate on the demands would enable efficient functioning of these companies."
      ],
      "metadata": {
        "id": "jiP96I2OE5a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Modules**"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#to display all the graph in the workbook\n",
        "sns.set_style(\"whitegrid\",{'grid.linestyle': '--'})"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the dataset**"
      ],
      "metadata": {
        "id": "5ahPXUtwFl_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's mount the google drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U3LL0HpHFrSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the file is in the same directory as the notebook:\n",
        "# df = pd.read_csv('SeoulBikeData.csv', encoding='latin1')\n",
        "\n",
        "# Or, if the file is in your Google Drive and you mounted it:\n",
        "df = pd.read_csv('/content/drive/MyDrive/SeoulBikeData.csv', encoding='latin1')\n",
        "# Replace 'MyDrive' with your Google Drive folder if needed.\n",
        "\n",
        "# Display the first few rows to verify\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Zzn-semKA5Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data shape\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data dtype\n",
        "df.info()"
      ],
      "metadata": {
        "id": "49UMn8xYG595"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical info\n",
        "df.describe(include='all').transpose()"
      ],
      "metadata": {
        "id": "iIhFdXTiHCtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing the data**"
      ],
      "metadata": {
        "id": "9VqZP_CxHNGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null values of data\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking duplicate\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "iDXs3XHoHh_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting date column dtype object to date\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') # Specifying the correct format"
      ],
      "metadata": {
        "id": "fFzlmyZUHxor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split day of week, month and year in three column\n",
        "df['day_of_week'] = df['Date'].dt.day_name() # extract week name from Date column\n",
        "df[\"month\"] = df['Date'].dt.month_name()   # extract month name from Date column\n",
        "df[\"year\"] = df['Date'].map(lambda x: x.year).astype(\"object\")     # extract year from Date column and convert it in object type"
      ],
      "metadata": {
        "id": "wR3o1xsKH3oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the Date column\n",
        "df.drop(columns=['Date'],inplace=True)"
      ],
      "metadata": {
        "id": "I_VEsp2RIC8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** EDA**"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When we observe the data we realize that Hour column is a numerical column but it is a time stamp so we have to treat Hour as a categorical feature"
      ],
      "metadata": {
        "id": "wTS9txZwIfVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Hour column integer to Categorical\n",
        "df['Hour']=df['Hour'].astype('object')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide Data in categorical and numerical features\n",
        "numeric_features= df.select_dtypes(exclude='object')\n",
        "categorical_features=df.select_dtypes(include='object')"
      ],
      "metadata": {
        "id": "wd4X6ipiI5cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features.head()"
      ],
      "metadata": {
        "id": "TUJAdPtyI-NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features.head()"
      ],
      "metadata": {
        "id": "vzNJ-INfJFzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking categorical column value count\n",
        "for i in categorical_features.columns:\n",
        "  print(\"\\n \")\n",
        "  print('column name  : ', i)\n",
        "  print(df[i].value_counts())"
      ],
      "metadata": {
        "id": "8rIv9De8JMAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting pairplot for more info\n",
        "sns.pairplot(df, corner=True)"
      ],
      "metadata": {
        "id": "jomR9f6SJSFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking Outliers with seaborn boxplot\n",
        "n = 1\n",
        "plt.figure(figsize=(20,15))\n",
        "\n",
        "for i in numeric_features.columns:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  sns.boxplot(df[i])\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "uzID-BpsJbuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we create point plots with Rented Bike Count during different categorical features with respect of Hour\n",
        "for i in categorical_features.columns:\n",
        "  if i == 'Hour':\n",
        "    pass\n",
        "  else:\n",
        "    plt.figure(figsize=(20,10))\n",
        "    sns.pointplot(x=df[\"Hour\"],y=df['Rented Bike Count'],hue=df[i])\n",
        "    plt.title(f\"Rented Bike Count during different {i} with respect of Hour\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "046ETSvZJkyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfODC_iNJ-hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observation**\n",
        "\n",
        "From all these pointplot we have observed a lot from every column like :\n",
        "\n",
        "# Season\n",
        "\n",
        "In the season column, we are able to understand that the demand is low in the winter season.\n",
        "\n",
        "# Holiday\n",
        "\n",
        "In the Holiday column, The demand is low during holidays, but in no holidays the demand is high, it may be because people use bikes to go to their work.\n",
        "\n",
        "# Functioning Day\n",
        "\n",
        "In the Functioning Day column, If there is no Functioning Day then there is no demand\n",
        "\n",
        "# Days of week\n",
        "\n",
        "In the Days of week column, We can observe from this column that the pattern of weekdays and weekends is different, in the weekend the demand becomes high in the afternoon. While the demand for office timings is high during weekdays, we can further change this column to weekdays and weekends.\n",
        "\n",
        "# month\n",
        "\n",
        "In the month column, We can clearly see that the demand is low in December January & Febuary, It is cold in these months and we have already seen in season column that demand is less in winters.\n",
        "\n",
        "# year\n",
        "\n",
        "The demand was less in 2017 and higher in 2018, it may be because it was new in 2017 and people did not know much about it."
      ],
      "metadata": {
        "id": "RKeDv0BMJ_MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Some more experiments for our categorical features**"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting days of weeks in Two variable from Monaday to Friday in Weekdays and Saturday and Sunday to Weekend\n",
        "df['week'] = df['day_of_week'].apply(lambda x:'Weekend'  if x=='Saturday' or  x== 'Sunday' else 'Weekdays')"
      ],
      "metadata": {
        "id": "mmnXU7I4KxAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value counts of Week column\n",
        "df.week.value_counts()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feel of week column with pointplot\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.pointplot(x=df[\"Hour\"],y=df['Rented Bike Count'],hue=df['week'])\n",
        "plt.title(\"Rented Bike Count during weekday and weekend with respect of Hour\")"
      ],
      "metadata": {
        "id": "KBSpsFCfLOmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we can clearly see the pattern which shows that the demand is high in the afternoon on the weekend. While there is more demand during office hours in weekdays"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we can drop the days of week column"
      ],
      "metadata": {
        "id": "P5Py6RSvLiGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# droping the days of week column from df and from categorical feature\n",
        "df.drop(columns=['day_of_week'], inplace=True)\n",
        "categorical_features.drop(columns=['day_of_week'], inplace=True)"
      ],
      "metadata": {
        "id": "YwKWQnhPLs8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **value Counts in percentage**"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in categorical_features.columns:\n",
        "  print('feature name : ',i)\n",
        "  print(df[i].value_counts(normalize=True))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating pieplot for all categorical feature\n",
        "n=1\n",
        "plt.figure(figsize=(20,15))\n",
        "for i in categorical_features.columns:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  plt.pie(df[i].value_counts(),labels = df[i].value_counts().keys().tolist(),autopct='%.0f%%')\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Now the time of Explore our numerical feature and Trying to take some important information from the Numeical feature**"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pays little attention to the skewness of our numerical features**"
      ],
      "metadata": {
        "id": "bMXWQWkcCJHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this plots we observe that some of our columns is right skewed and some are left skewed we have to remember this things when we apply algorithms\n",
        "\n",
        "# Right skewed columns are\n",
        "\n",
        "Rented Bike Count (Its also our Dependent variable), Wind speed (m/s), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm),\n",
        "\n",
        "# Left skewed columns are\n",
        "\n",
        "Visibility (10m), Dew point temperature(°C)"
      ],
      "metadata": {
        "id": "ozWdJ_pyCW7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's try something else to get information from our Numerical features**"
      ],
      "metadata": {
        "id": "aDfgy2L3DFu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting histogram with mean and median, and distplot of all the numeric features of the dataset\n",
        "n=1\n",
        "for i in numeric_features.columns:\n",
        "  plt.figure(figsize=(20,40))\n",
        "  plt.subplot(9,2,n)\n",
        "  n+=1\n",
        "  print('\\n')\n",
        "  print('='*70,i,'='*70)\n",
        "  print('\\n')\n",
        "  # fig=plt.figure()\n",
        "  # ax=fig.gca()\n",
        "  feature=df[i]\n",
        "  feature.hist(bins=50,)\n",
        "  plt.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  plt.subplot(9,2,n)\n",
        "  n+= 1\n",
        "  sns.distplot(df[i])\n",
        "  # plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In Distplot plots we observe that some of our columns is right skewed and some are left skewed we have to remember this things when we apply algorithms\n",
        "\n",
        "# Right skewed columns are\n",
        "\n",
        "Rented Bike Count (Its also our Dependent variable), Wind speed (m/s), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm),\n",
        "\n",
        "# Left skewed columns are\n",
        "\n",
        "Visibility (10m), Dew point temperature(°C)\n",
        "\n",
        "# From Histogram we are coming to know that the features which are skewed, their mean and the median are also skewed, which was understood by looking at the graph that this would happen.\n",
        "\n"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lets try to find how is the relation of numerical features with our dependent variable**"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression plot to know relation with our independent variable\n",
        "n=1\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in numeric_features.columns:\n",
        "  if i == 'Rented Bike Count':\n",
        "    pass\n",
        "  else:\n",
        "    plt.subplot(4,2,n)\n",
        "    n+=1\n",
        "    # Pass data as a single argument using the 'data' parameter\n",
        "    # Specify x and y variables using the 'x' and 'y' parameters\n",
        "    sns.regplot(data=df, x=i, y='Rented Bike Count', scatter_kws={\"color\": \"cyan\"}, line_kws={\"color\": \"red\"})\n",
        "    plt.title(f'Dependend variable and {i}')\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "xmrXDB3hFDKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This regression plots shows that some of our features are positive linear and some are negative linear in relation to our target variable."
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Now is the time to know what is the correlation of our dependent variable with the independent features**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with Rented Bike Count, considering only numeric columns\n",
        "df.corr(numeric_only=True)['Rented Bike Count']"
      ],
      "metadata": {
        "id": "TCa6m5AxGENZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As we saw in the regression plot that some features are negatively correlated and some positive, we are seeing the same thing here as well."
      ],
      "metadata": {
        "id": "50PfTAjWGShH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let us see the correlation of all the numerical features with the heat map, so that we will also get to know the multicolinearity.**"
      ],
      "metadata": {
        "id": "oH4nwDk9GcmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using seaborn heatmap for ploting correlation graph\n",
        "plt.figure(figsize=(10,8))\n",
        "# Calculate correlation only for numeric columns\n",
        "sns.heatmap(abs(df.corr(numeric_only=True)), cmap='coolwarm', annot=True)"
      ],
      "metadata": {
        "id": "08Egy1X0G27x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From this graph we are able to see that there is multicollinearity in temperature(°C) and dev point temperature(°C) column."
      ],
      "metadata": {
        "id": "QXGavG1VHDUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "   # Calculating VIF\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "   return(vif)"
      ],
      "metadata": {
        "id": "3lqMRmLtHOp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Rented Bike Count','Dew point temperature(°C)']]])"
      ],
      "metadata": {
        "id": "edudBxWmHXXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pandas get Dummies for Encoding categorical features\n",
        "new_df=pd.get_dummies(df,drop_first=True,sparse=True)"
      ],
      "metadata": {
        "id": "DgqdlHaqHcq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(2)"
      ],
      "metadata": {
        "id": "DdTM8qDNHeq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We saw that our dependent variable is right skewed, it needs to be normalized.**\n",
        "## We do some experiments to normalize it"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,3,figsize=(20,5))\n",
        "# here we use log10\n",
        "sns.distplot(np.log10(new_df['Rented Bike Count']+0.0000001),ax=axes[0],color='red').set_title(\"log 10\")\n",
        "# here we use square\n",
        "sns.distplot((new_df['Rented Bike Count']**2),ax=axes[1],color='red').set_title(\"square\")\n",
        "# here we use square root\n",
        "sns.distplot(np.sqrt(new_df['Rented Bike Count']),ax=axes[2], color='green').set_title(\"Square root\")"
      ],
      "metadata": {
        "id": "PSzLEWjRH96c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our data in green plot is normalized to some extent: so we will go with square root on our dependent variable"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divide data in dependent feature and Independent feature"
      ],
      "metadata": {
        "id": "H3hQNfIjIpsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_df.drop(columns=['Rented Bike Count','Dew point temperature(°C)'])\n",
        "y = np.sqrt(new_df['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split our data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.25,random_state=42)"
      ],
      "metadata": {
        "id": "1uYAJvvOI2UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Geeting Feel of my X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "J3I5VKnZJQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "WeYorW0uJXxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.head()"
      ],
      "metadata": {
        "id": "Q2mLkEm6JeDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "tfOC3YdnJhZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending all models parameters to the corrosponding list\n",
        "mean_absolut_error = []\n",
        "mean_sq_error=[]\n",
        "root_mean_sq_error=[]\n",
        "training_score =[]\n",
        "r2_list=[]\n",
        "adj_r2_list=[]\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "def score_metrix (model,X_train,X_test,Y_train,Y_test):\n",
        "\n",
        "  '''\n",
        "    train the model and gives mae, mse,rmse,r2,adj r2 score of the model\n",
        "\n",
        "  '''\n",
        "  #training the model\n",
        "  model.fit(X_train,Y_train)\n",
        "\n",
        "  # Training Score\n",
        "  training  = model.score(X_train,Y_train)\n",
        "  print(\"Training score  =\", training)\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "  try:\n",
        "      # finding the best parameters of the model if any\n",
        "    print('*'*20, 'Best Parameters & Best Score', '*'*20)\n",
        "    print(f\"The best parameters found out to be :{model.best_params_} \\nwhere model best score is:  {model.best_score_} \\n\")\n",
        "  except:\n",
        "    print('None')\n",
        "\n",
        "\n",
        "\n",
        "  #predicting the Test set and evaluting the models\n",
        "  print('\\n')\n",
        "  print('*'*20, 'Evalution Matrix', '*'*20)\n",
        "\n",
        "  if model == Linear or model == L1 or model == L2:\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test**2,Y_pred**2)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test**2,Y_pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "  else:\n",
        "    # for tree base models\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test,Y_pred)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test,Y_pred)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test,Y_pred)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test,Y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "    #Top 10 feature importance graph\n",
        "    try:\n",
        "      best = model.best_estimator_\n",
        "      features = new_X.columns\n",
        "      importances = best.feature_importances_[0:10]\n",
        "      indices = np.argsort(importances)\n",
        "      plt.figure(figsize=(10,15))\n",
        "      plt.title('Feature Importance')\n",
        "      plt.barh(range(len(indices)), importances[indices], color='pink',edgecolor='red' ,align='center')\n",
        "      plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "      plt.xlabel('Relative Importance')\n",
        "      plt.show()\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Here we appending the parameters for all models\n",
        "  mean_absolut_error.append(MAE)\n",
        "  mean_sq_error.append(MSE)\n",
        "  root_mean_sq_error.append(RMSE)\n",
        "  training_score.append(training)\n",
        "  r2_list.append(r2)\n",
        "  adj_r2_list.append(adj_r2)\n",
        "\n",
        "\n",
        "  # print the cofficient and intercept of which model have these parameters and else we just pass them\n",
        "  if model == Linear:\n",
        "    print(\"*\"*25, \"coefficient\", \"*\"*25)\n",
        "    print(model.coef_)\n",
        "    print('\\n')\n",
        "    print(\"*\"*25, \"Intercept\", \"*\"*25)\n",
        "    print('\\n')\n",
        "    print(model.intercept_)\n",
        "  else:\n",
        "    pass\n",
        "  print('\\n')\n",
        "\n",
        "  print('*'*20, 'ploting the graph of Actual and predicted only with 80 observation', '*'*20)\n",
        "\n",
        "  # ploting the graph of Actual and predicted only with 80 observation for better visualisation which model have these parameters and else we just pass them\n",
        "  try:\n",
        "    # ploting the line graph of actual and predicted values\n",
        "    plt.figure(figsize=(15,7))\n",
        "    plt.plot((Y_pred)[:80])\n",
        "    plt.plot((np.array(Y_test)[:80]))\n",
        "    plt.legend([\"Predicted\",\"Actual\"])\n",
        "    plt.show()\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "qVqcJy7TQUeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transforming X_train and X_test with yeo-johnson transformation"
      ],
      "metadata": {
        "id": "9e9q_Yd7Jp1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer,MinMaxScaler\n",
        "yeo = PowerTransformer()\n",
        "X_train_trans = yeo.fit_transform(X_train) # fit transform the training set\n",
        "X_test_trans = yeo.transform(X_test) #tranform the test set"
      ],
      "metadata": {
        "id": "JTu_IN2HQsyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**"
      ],
      "metadata": {
        "id": "4wWj8vRJQyj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imporing linear models\n",
        "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
        "Linear = LinearRegression()"
      ],
      "metadata": {
        "id": "bGL-GBrBQ677"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Fitting the linear regression model with our score matrix function\n",
        "score_metrix(Linear,X_train_trans,X_test_trans,y_train,y_test)"
      ],
      "metadata": {
        "id": "9cwENiM5RCAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RandomForest Regression**"
      ],
      "metadata": {
        "id": "4jRGibpYRKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Randomfroest from sklearn.ensemble\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "jk9nRWPLRP0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators':[100,150,200],\n",
        "              'min_samples_leaf':[6,4,2],\n",
        "              'max_depth' : [30,20,25],\n",
        "              'min_samples_split': [30,25,20],\n",
        "              'max_features':['auto','sqrt','log2']\n",
        "              }"
      ],
      "metadata": {
        "id": "9LUAyz6ERQdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Grid SearchCV\n",
        "Ranom_forest_Grid_search = GridSearchCV(RandomForestRegressor(),param_grid=param_grid,n_jobs=-1,cv=5)"
      ],
      "metadata": {
        "id": "PrjzrNSLRQ6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGBRegressor**"
      ],
      "metadata": {
        "id": "NDWSPXhORlcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing XGBoost Regressor\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "JGgzWoKQRrYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'subsample': [0.5],#0.3,0.7],\n",
        "          'n_thread': [4], #2,6],\n",
        "          'n_estimators': [1000],#range(200,1500,50),\n",
        "          'min_child_weight': [2],#3,5],\n",
        "          'max_depth': [4],#range(2,8,2),\n",
        "          'learning_rate': [0.02],#0.04,0.06],\n",
        "          'eval_mertric': ['rmse'],#'mse',],\n",
        "          'colsample_bytree': [0.7],#0.5,1.0],\n",
        "          }"
      ],
      "metadata": {
        "id": "YB1cNoZ6Rr2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating xgb grid model\n",
        "xgb_grid_search= GridSearchCV(XGBRegressor(silent=True),param_grid=params,cv=5)"
      ],
      "metadata": {
        "id": "N09EZ7cFR5F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}